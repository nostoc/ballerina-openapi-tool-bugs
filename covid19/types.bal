// AUTO-GENERATED FILE. DO NOT MODIFY.
// This file is auto-generated by the Ballerina OpenAPI tool.

import ballerina/constraint;
import ballerina/data.jsondata;
import ballerina/http;

# The quality of the image that will be generated
public type ImageQuality "standard"|"hd";

public type InlineResponse2001Data record {
    int index;
    decimal[] embedding;
    string 'object;
};

public type ContentFilterIdResult record {
    *ContentFilterResultBase;
    string id;
};

public type ChatCompletionRequestMessage record {
    # The role of the messages author
    ChatCompletionRequestMessageRole role;
};

public type ContentFilterDetectedResult record {
    *ContentFilterResultBase;
    boolean detected;
};

public type SpanPolygon record {
    # The x-coordinate of the point
    decimal x?;
    # The y-coordinate of the point
    decimal y?;
};

# The grounding enhancement that returns the bounding box of the objects detected in the image
public type EnhancementGrounding record {
    Line[] lines;
};

# `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function
public type ChatCompletionToolChoiceOptionOneOf1 "none"|"auto";

# Information about the content filtering results
public type ContentFilterResultsBase record {
    @jsondata:Name {value: "self_harm"}
    ContentFilterSeverityResult selfHarm?;
    @jsondata:Name {value: "custom_blocklists"}
    ContentFilterIdResult[] customBlocklists?;
    ContentFilterSeverityResult hate?;
    ContentFilterDetectedResult profanity?;
    ErrorBase 'error?;
    ContentFilterSeverityResult sexual?;
    ContentFilterSeverityResult violence?;
};

# Transcription request
public type CreateTranscriptionRequest record {
    # The audio file object to transcribe
    record {byte[] fileContent; string fileName;} file;
    @jsondata:Name {value: "response_format"}
    AudioResponseFormat responseFormat?;
    # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit
    decimal temperature = 0;
    # The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency
    string language?;
    # An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language
    string prompt?;
};

public type InlineResponse200Usage record {
    @jsondata:Name {value: "completion_tokens"}
    decimal completionTokens;
    @jsondata:Name {value: "prompt_tokens"}
    decimal promptTokens;
    @jsondata:Name {value: "total_tokens"}
    decimal totalTokens;
};

# Controls which (if any) function is called by the model. `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function. Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that function
public type ChatCompletionToolChoiceOption ChatCompletionToolChoiceOptionOneOf1|ChatCompletionNamedToolChoice;

public type GenerateImagesResponse record {
    # The result data of the operation, if successful
    ImageResult[] data?;
    # The unix timestamp when the operation was created
    int created;
    Error 'error?;
};

# Represents the Queries record for the operation: embeddings_create
public type EmbeddingsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# Translation request
public type CreateTranslationRequest record {
    # The audio file to translate
    record {byte[] fileContent; string fileName;} file;
    @jsondata:Name {value: "response_format"}
    AudioResponseFormat responseFormat?;
    # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit
    decimal temperature = 0;
    # An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English
    string prompt?;
};

# The style of the generated images
public type ImageStyle "vivid"|"natural";

public type Enhancement record {
    # The grounding enhancement that returns the bounding box of the objects detected in the image
    EnhancementGrounding grounding?;
};

# Provides a set of configurations for controlling the behaviours when communicating with a remote HTTP endpoint.
@display {label: "Connection Config"}
public type ConnectionConfig record {|
    # Provides Auth configurations needed when communicating with a remote HTTP endpoint.
    http:BearerTokenConfig|ApiKeysConfig auth;
    # The HTTP version understood by the client
    http:HttpVersion httpVersion = http:HTTP_2_0;
    # Configurations related to HTTP/1.x protocol
    http:ClientHttp1Settings http1Settings = {};
    # Configurations related to HTTP/2 protocol
    http:ClientHttp2Settings http2Settings = {};
    # The maximum time to wait (in seconds) for a response before closing the connection
    decimal timeout = 30;
    # The choice of setting `forwarded`/`x-forwarded` header
    string forwarded = "disable";
    # Configurations associated with Redirection
    http:FollowRedirects followRedirects?;
    # Configurations associated with request pooling
    http:PoolConfiguration poolConfig?;
    # HTTP caching related configurations
    http:CacheConfig cache = {};
    # Specifies the way of handling compression (`accept-encoding`) header
    http:Compression compression = http:COMPRESSION_AUTO;
    # Configurations associated with the behaviour of the Circuit Breaker
    http:CircuitBreakerConfig circuitBreaker?;
    # Configurations associated with retrying
    http:RetryConfig retryConfig?;
    # Configurations associated with cookies
    http:CookieConfig cookieConfig?;
    # Configurations associated with inbound response size limits
    http:ResponseLimitConfigs responseLimits = {};
    # SSL/TLS-related options
    http:ClientSecureSocket secureSocket?;
    # Proxy server related options
    http:ProxyConfig proxy?;
    # Provides settings related to client socket configuration
    http:ClientSocketConfig socketConfig = {};
    # Enables the inbound payload validation functionality which provided by the constraint package. Enabled by default
    boolean validation = true;
    # Enables relaxed data binding on the client side. When enabled, `nil` values are treated as optional, 
    # and absent fields are handled as `nilable` types. Enabled by default.
    boolean laxDataBinding = true;
|};

# Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about third party text and profanity, if it has been detected, and if it has been filtered or not. And information about customer block list, if it has been filtered and its id
public type ContentFilterChoiceResults record {
    *ContentFilterResultsBase;
    *ContentFilterChoiceResultsAllOf2;
    *ContentFilterChoiceResultsContentFilterChoiceResultsAllOf23;
};

public type InlineResponse200Choices record {
    @jsondata:Name {value: "content_filter_results"}
    ContentFilterChoiceResults contentFilterResults?;
    @jsondata:Name {value: "finish_reason"}
    string finishReason?;
    int index?;
    string text?;
    InlineResponse200Logprobs? logprobs?;
};

# The type of enhancements needed.
public type ExtensionsChatCompletionsRequest_enhancements record {
    # Request object to specify if grounding enhancement is needed.
    ExtensionsChatCompletionsRequest_enhancements_grounding grounding?;
    # Request object to specify if ocr enhancement is needed.
    ExtensionsChatCompletionsRequest_enhancements_ocr ocr?;
};

# The type of the tool. Currently, only `function` is supported
public type ChatCompletionToolType "function";

# Represents the Queries record for the operation: ExtensionsChatCompletions_Create
public type ExtensionsChatCompletionsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer block list, if it has been filtered and its id
public type ContentFilterPromptResults record {
    *ContentFilterResultsBase;
    ContentFilterDetectedResult jailbreak?;
};

# Setting to `json_object` enables JSON mode. This guarantees that the message the model generates is valid JSON
public type ChatCompletionResponseFormat "text"|"json_object"?;

# Request object to specify if grounding enhancement is needed.
public type ExtensionsChatCompletionsRequest_enhancements_grounding record {
    boolean enabled = false;
};

public type InputItemsString string;

# The type of the tool call, in this case `function`
public type ToolCallType "function";

public type ChatCompletionTool record {
    ChatCompletionToolFunction 'function;
    # The type of the tool. Currently, only `function` is supported
    ChatCompletionToolType 'type;
};

public type ContentFilterChoiceResultsAllOf2 record {
    ContentFilterDetectedResult protected_material_text?;
};

# An object specifying the format that the model must output. Used to enable JSON mode.
public type CreateChatCompletionRequest_response_format record {
    # Setting to `json_object` enables JSON mode. This guarantees that the message the model generates is valid JSON
    ChatCompletionResponseFormat? 'type?;
};

# Represents the Queries record for the operation: ChatCompletions_Create
public type ChatCompletionsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# The data source to be used for the Azure OpenAI on your data feature
public type DataSource record {
    # The data source type
    string 'type;
    # The parameters to be used for the data source in runtime
    record {} parameters?;
};

# A content line object consisting of an adjacent sequence of content elements, such as words and selection marks
public type Line record {
    # An array of spans that represent detected objects and its bounding box information
    Span[] spans;
    string text;
};

# Represents the Queries record for the operation: Translations_Create
public type TranslationsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# The image url or encoded image if successful, and an error otherwise
public type ImageResult record {
    # The prompt that was used to generate the image, if there was any revision to the prompt
    @jsondata:Name {value: "revised_prompt"}
    string revisedPrompt?;
    # The base64 encoded image
    @jsondata:Name {value: "b64_json"}
    string b64Json?;
    # The image url
    string url?;
};

public type ContentFilterChoiceResultsContentFilterChoiceResultsAllOf23 record {
    ContentFilterDetectedWithCitationResult protected_material_code?;
};

# Defines the format of the output
public type AudioResponseFormat "json"|"text"|"srt"|"verbose_json"|"vtt";

public type ExtensionsChatCompletionChoice record {
    *ChatCompletionChoiceCommon;
    Enhancement enhancements?;
    # A chat message
    Message message?;
};

public type ContentFilterSeverityResult record {
    *ContentFilterResultBase;
    "safe"|"low"|"medium"|"high" severity;
};

public type InlineResponse2001Usage record {
    @jsondata:Name {value: "prompt_tokens"}
    int promptTokens;
    @jsondata:Name {value: "total_tokens"}
    int totalTokens;
};

public type DeploymentIdCompletionsBody record {
    # Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {"50256" &#58; -100} to prevent the <|endoftext|> token from being generated
    @jsondata:Name {value: "logit_bias"}
    record {} logitBias?;
    # The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0
    @jsondata:Name {value: "max_tokens"}
    int? maxTokens = 16;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics
    @jsondata:Name {value: "presence_penalty"}
    decimal presencePenalty = 0;
    # Echo back the prompt in addition to the completion
    boolean? echo = false;
    # The suffix that comes after a completion of inserted text
    string? suffix?;
    # How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
    # Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop
    int? n = 1;
    # Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
    # Minimum of 0 and maximum of 5 allowed
    int? logprobs?;
    # An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    # We generally recommend altering this or temperature but not both
    @jsondata:Name {value: "top_p"}
    decimal? topp = 1;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim
    @jsondata:Name {value: "frequency_penalty"}
    decimal frequencyPenalty = 0;
    # Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence
    string|string[]? stop?;
    # Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
    # When used with n, best_of controls the number of candidate completions and n specifies how many to return - best_of must be greater than n.
    # Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128
    @jsondata:Name {value: "best_of"}
    int bestOf?;
    # Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message
    boolean? 'stream = false;
    # What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
    # We generally recommend altering this or top_p but not both
    decimal? temperature = 1;
    @jsondata:Name {value: "completion_config"}
    string? completionConfig?;
    # The prompt(s) to generate completions for, encoded as a string or array of strings.
    # Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048
    string|string[]? prompt?;
    # A unique identifier representing your end-user, which can help monitoring and detecting abuse
    string user?;
};

# Provides API key configurations needed when communicating with a remote HTTP endpoint.
public type ApiKeysConfig record {|
    string apiKey;
|};

# Translation or transcription response when response_format was json
public type AudioResponse record {
    # Translated or transcribed text
    string text;
};

public type DeploymentIdEmbeddingsBody record {
    # Input text to get embeddings for, encoded as a string. To get embeddings for multiple inputs in a single request, pass an array of strings. Each input must not exceed 2048 tokens in length.
    # Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space, as we have observed inferior results when newlines are present.
    string|InputItemsString[]? input;
    # A unique identifier representing your end-user, which can help monitoring and detecting abuse.
    string user?;
    # input type of embedding search to use
    string input_type?;
};

# Specifies a tool the model should use. Use to force the model to call a specific function
public type ChatCompletionNamedToolChoice record {
    ChatCompletionNamedToolChoiceFunction 'function?;
    # The type of the tool. Currently, only `function` is supported
    "function" 'type?;
};

# A chat message
public type Message record {
    # The role of the author of this message
    "system"|"user"|"assistant"|"tool" role;
    # The recipient of the message in the format of <namespace>.<operation>. Present if and only if the recipient is tool
    string recipient?;
    # The conversation context
    MessageContext? context?;
    # The index of the message in the conversation
    int index?;
    # Whether the message ends the turn
    @jsondata:Name {value: "end_turn"}
    boolean endTurn?;
    # The contents of the message
    string content;
};

# The size of the generated images
public type ImageSize "1792x1024"|"1024x1792"|"1024x1024";

public type ContentFilterResultBase record {
    boolean filtered;
};

public type InlineResponse200 record {
    int created;
    @jsondata:Name {value: "prompt_filter_results"}
    PromptFilterResults promptFilterResults?;
    InlineResponse200Usage usage?;
    string model;
    string id;
    InlineResponse200Choices[] choices;
    string 'object;
};

# Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model
public type ChatCompletionFunctionCall record {
    # The name of the function to call
    string name;
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function
    string arguments;
};

# A span object that represents a detected object and its bounding box information
public type Span record {
    # The character offset within the text where the span begins. This offset is defined as the position of the first character of the span, counting from the start of the text as Unicode codepoints
    int offset;
    # An array of objects representing points in the polygon that encloses the detected object
    SpanPolygon[] polygon;
    # The length of the span in characters, measured in Unicode codepoints
    int length;
    # The text content of the span that represents the detected object
    string text;
};

# The role of the messages author
public type ChatCompletionRequestMessageRole "system"|"user"|"assistant"|"tool"|"function";

public type ChatCompletionChoiceCommon record {
    @jsondata:Name {value: "finish_reason"}
    string finishReason?;
    int index?;
};

public type CreateChatCompletionRequest record {
    *ChatCompletionsRequestCommon;
    # An object specifying the format that the model must output. Used to enable JSON mode.
    CreateChatCompletionRequest_response_format response_format?;
    # If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
    int? seed = 0;
    # Deprecated in favor of `tools`. A list of functions the model may generate JSON inputs for.
    ChatCompletionFunction[] functions?;
    # Deprecated in favor of `tool_choice`. Controls how the model responds to function calls. "none" means the model does not call a function, and responds to the end-user. "auto" means the model can pick between an end-user or calling a function.  Specifying a particular function via `{"name":\ "my_function"}` forces the model to call that function. "none" is the default when no functions are present. "auto" is the default if functions are present.
    @jsondata:Name {value: "function_call"}
    "none"|"auto"|record {string name;} functionCall?;
    # A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
    ChatCompletionRequestMessage[] messages;
    @jsondata:Name {value: "tool_choice"}
    ChatCompletionToolChoiceOption toolChoice?;
    # A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
    ChatCompletionTool[] tools?;
    # How many chat completion choices to generate for each input message.
    int? n = 1;
};

public type ChatCompletionsResponseCommon record {
    # The Unix timestamp (in seconds) of when the chat completion was created
    int created;
    # Usage statistics for the completion request
    CompletionUsage usage?;
    # The model used for the chat completion
    string model;
    # A unique identifier for the chat completion
    string id;
    # Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism
    @jsondata:Name {value: "system_fingerprint"}
    string systemFingerprint?;
    # The object type
    ChatCompletionResponseObject 'object;
};

# Represents the Queries record for the operation: Completions_Create
public type CompletionsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# The format in which the generated images are returned
public type ImagesResponseFormat "url"|"b64_json";

public type ErrorBase record {
    string code?;
    string message?;
};

public type ChatCompletionsRequestCommon record {
    # An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    # We generally recommend altering this or `temperature` but not both
    @jsondata:Name {value: "top_p"}
    decimal? topp = 1;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim
    @jsondata:Name {value: "frequency_penalty"}
    decimal frequencyPenalty = 0;
    # Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token
    @jsondata:Name {value: "logit_bias"}
    record {}? logitBias?;
    # Up to 4 sequences where the API will stop generating further tokens
    string|string[]? stop = ();
    # If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message
    boolean? 'stream = false;
    # The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens)
    @jsondata:Name {value: "max_tokens"}
    int maxTokens = 4096;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics
    @jsondata:Name {value: "presence_penalty"}
    decimal presencePenalty = 0;
    # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    # We generally recommend altering this or `top_p` but not both
    decimal? temperature = 1;
    # A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse
    string user?;
};

# The conversation context
public type MessageContext record {
    # Messages exchanged between model and extensions prior to final message from model
    Message[]? messages?;
};

# The role of the author of the response message
public type ChatCompletionResponseMessageRole "assistant";

# Usage statistics for the completion request
public type CompletionUsage record {
    # Number of tokens in the generated completion
    @jsondata:Name {value: "completion_tokens"}
    int completionTokens;
    # Number of tokens in the prompt
    @jsondata:Name {value: "prompt_tokens"}
    int promptTokens;
    # Total number of tokens used in the request (prompt + completion)
    @jsondata:Name {value: "total_tokens"}
    int totalTokens;
};

# The object type
public type ChatCompletionResponseObject "chat.completion";

# The response of the extensions chat completions
public type ExtensionsChatCompletionsResponse record {
    *ChatCompletionsResponseCommon;
    ExtensionsChatCompletionChoice[] choices?;
};

# Content filtering results for a single prompt in the request
public type PromptFilterResult record {
    @jsondata:Name {value: "content_filter_results"}
    ContentFilterPromptResults contentFilterResults?;
    @jsondata:Name {value: "prompt_index"}
    int promptIndex?;
};

# Represents the Queries record for the operation: ImageGenerations_Create
public type ImageGenerationsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# Content filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders
public type PromptFilterResults PromptFilterResult[];

public type CreateChatCompletionResponse record {
    *ChatCompletionsResponseCommon;
    @jsondata:Name {value: "prompt_filter_results"}
    PromptFilterResults promptFilterResults?;
    record {*ChatCompletionChoiceCommon; @jsondata:Name {value: "content_filter_results"}
        ContentFilterChoiceResults contentFilterResults?; ChatCompletionResponseMessage message?;}[] choices;
};

public type Error ErrorBase;

# The function that the model called
public type ChatCompletionMessageToolCallFunction record {
    # The name of the function to call
    string name;
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function
    string arguments;
};

# Represents the Queries record for the operation: Transcriptions_Create
public type TranscriptionsCreateQueries record {
    # api version
    @http:Query {name: "api-version"}
    string apiVersion;
};

# Translation or transcription response when response_format was verbose_json
public type AudioVerboseResponse record {
    *AudioResponse;
    # Duration.
    decimal duration?;
    # Type of audio task.
    "transcribe"|"translate" task?;
    # Language.
    string language?;
    AudioSegment[] segments?;
};

public type ChatCompletionToolFunction record {
    # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64
    string name;
    # A description of what the function does, used by the model to choose when and how to call the function
    string description?;
    # The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format
    ChatCompletionFunctionParameters parameters;
};

public type ChatCompletionMessageToolCall record {
    # The function that the model called
    ChatCompletionMessageToolCallFunction 'function;
    # The ID of the tool call
    string id;
    # The type of the tool call, in this case `function`
    ToolCallType 'type;
};

public type ContentFilterDetectedWithCitationResult record {
    *ContentFilterDetectedResult;
    boolean filtered;
    ContentFilterDetectedWithCitationResult_citation citation?;
};

public type InlineResponse200Logprobs record {
    @jsondata:Name {value: "top_logprobs"}
    record {||}[] topLogprobs?;
    @jsondata:Name {value: "token_logprobs"}
    decimal[] tokenLogprobs?;
    string[] tokens?;
    @jsondata:Name {value: "text_offset"}
    int[] textOffset?;
};

# A chat completion message generated by the model
public type ChatCompletionResponseMessage record {
    # The role of the author of the response message
    ChatCompletionResponseMessageRole role?;
    @jsondata:Name {value: "function_call"}
    ChatCompletionFunctionCall functionCall?;
    # The tool calls generated by the model, such as function calls
    @jsondata:Name {value: "tool_calls"}
    ChatCompletionMessageToolCall[] toolCalls?;
    # The contents of the message
    string? content?;
};

public type ContentFilterDetectedWithCitationResult_citation record {
    string license?;
    @jsondata:Name {value: "URL"}
    string uRL?;
};

public type ImageGenerationsRequest record {
    @jsondata:Name {value: "response_format"}
    ImagesResponseFormat responseFormat?;
    # The size of the generated images
    ImageSize size?;
    # The style of the generated images
    ImageStyle style?;
    # A text description of the desired image(s). The maximum length is 4000 characters
    @constraint:String {minLength: 1}
    string prompt;
    # A unique identifier representing your end-user, which can help to monitor and detect abuse
    string user?;
    # The number of images to generate
    @constraint:Int {minValue: 1, maxValue: 1}
    int n = 1;
    # The quality of the image that will be generated
    ImageQuality quality?;
};

public type ChatCompletionNamedToolChoiceFunction record {
    # The name of the function to call
    string name;
};

# Request object to specify if ocr enhancement is needed.
public type ExtensionsChatCompletionsRequest_enhancements_ocr record {
    boolean enabled = false;
};

public type InlineResponse2001 record {
    InlineResponse2001Data[] data;
    InlineResponse2001Usage usage;
    string model;
    string 'object;
};

# Request for the chat completions using extensions
public type ExtensionsChatCompletionsRequest record {
    *ChatCompletionsRequestCommon;
    # The type of enhancements needed.
    ExtensionsChatCompletionsRequest_enhancements enhancements?;
    Message[] messages;
    # The data sources to be used for the Azure OpenAI on your data feature.
    DataSource[] dataSources?;
};

public type InlineResponse2002 AudioResponse|AudioVerboseResponse;

public type ChatCompletionFunction record {
    # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64
    string name;
    # The description of what the function does
    string description?;
    # The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format
    ChatCompletionFunctionParameters parameters?;
};

# The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format
public type ChatCompletionFunctionParameters record {
};

# Transcription or translation segment
public type AudioSegment record {
    # Segment start offset
    decimal 'start?;
    # Temperature
    decimal temperature?;
    # Average log probability
    @jsondata:Name {value: "avg_logprob"}
    decimal avgLogprob?;
    # Probability of 'no speech'
    @jsondata:Name {value: "no_speech_prob"}
    decimal noSpeechProb?;
    # Segment end offset
    decimal end?;
    # Tokens of the text
    decimal[] tokens?;
    # Segment identifier
    int id?;
    # Segment text
    string text?;
    # Offset of the segment
    decimal seek?;
    # Compression ratio
    @jsondata:Name {value: "compression_ratio"}
    decimal compressionRatio?;
};
